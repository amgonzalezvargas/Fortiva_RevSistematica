Regarding the sensors used in the different studies, there is a clear tendency to use cameras as primary sensors. All 50 studies (100%) mentioned the use of cameras in their research. However, other sensing technologies were used to support the incoming information from the cameras. Six studies incorporated inertial measurement units (IMUs) or similar motion sensors to enhance navigation and localization (Bai et al., 2019; Lee & Medioni, 2015; Li et al., 2019; Mekhalfi et al., 2015; Tian et al., 2021; Zhang & Ye, 2020). Ultrasonic sensors were also employed for obstacle detection in five studies (Bai et al., 2018; Khairnar et al., 2020; Mocanu et al., 2016; Rao & Singh, 2021; Ryan et al., 2019). GPS modules were used in four studies to aid in outdoor navigation (Chen et al., 2019; Katz et al., 2012; Ranaweera et al., 2017; Slade et al., 2021). Some studies incorporated specialized sensors like time-of-flight lasers (Mekhalfi et al., 2015) or infrared sensors (Ranaweera et al., 2017; Ryan et al., 2019). The types of cameras were RGBD, Smart-phone embedded, stereo cameras, webcams, Raspberry modules, fisheye, TOF cameras, Google Glass, GoPro, among others. Figure X shows the number of papers that utilised different cameras and other sensors. RGB-D cameras were used in 13 studies (26%). Among those that employed RGB-D cameras, the most frequently used brands were Intel RealSense (Ackattupathil et al., 2020; Bai et al., 2019; Tian et al., 2021; Zhang & Ye, 2020; Zhang et al., 2022) and Asus Xtion Pro (Aladrén et al., 2014; Barontini et al., 2020; Martinez et al., 2017). Another camera mentioned is Primesense (Lee & Medioni, 2011). Li et al., (2019) used an RGB-D camera that was part of the Google Tango device, which also provided a wide angle camera. The specifications of these cameras varied, with resolutions ranging from 320x240 to 1280x720 pixels and frame rates from 15 to 100 fps. Some studies (Bai et al., 2020; Chen et al., 2021; Lee & Medioni, 2015) used RGB-D cameras but did not specify the brand or model.Smartphone or iPhone cameras were also widely used, in 12 studies (24%). A variety of brands and models were used, including Samsung Galaxy S7 (Mocanu et al., 2016), iPhone (Ko & Kim, 2017, Hu et al., 2015), Infocus M320 (Lin et al., 2017), Nokia N95 (Manduchi, 2012), HUAWEI P20 (Zhang et al., 2019), and HTC One M8 (Zheng & Weng, 2016). Abobeah et al. (2018) used the HTC Desire 820 G PLUS dual sim-chest-mounted-camera with 13 megapixels, (720 × 1280) resolution, and (1080p@30fps) video camera. Some studies (Chen et al., 2019; Khairnar et al., 2020; Ritterbusch & Jaworek, 2018; Yang & Saniie, 2020) used smartphone cameras but did not specify the brand or model. Camera specifications varied widely, with resolutions ranging from 320x240 to 1080x1920 pixels and frame rates up to 25 fps.The perception module of an assistive device aims to extract a digital description of the user’s scene from raw sensor data, that the planning system can rely on to generate correct navigation instructions. This description should represent different aspects of the scene, including the built infrastructure, the pose of the user, and the pose of other social agents. The built infrastructure representation in particular should be parsed at least into occupied and unoccupied areas, which allows the assistive device to know what portions of the environment are traversable. This section describes how different assistive navigation technologies approach the aforementioned tasks. We start by reviewing how assistive navigation technologies solve the coupled tasks of creating a digital representation of the built infrastructure and estimating the position of the user. The former task is commonly known as mapping, whereas the latter is called localisation. The Simultaneous Localisation and Mapping (SLAM) problem aims to jointly solve both problems. Mapping and localization are critical components of independent navigation systems for individuals with visual disabilities. Estimating correct and deliberate instructions for the user hinges on knowing their pose and that of the obstacles around them. This requirement can be relaxed when providing the user instantaneous local instructions based on free path  (Martinez et al., 2017), crosswalk and stair detections (Sövény, Kovács, & Kardkovács, 2014; Ritterbusch & Jaworek, 2018) or based on the locations of the obstacles the user encounters  (Lin, Lee, & Chiang, 2017), thus avoiding the need for user localisation. However, global localisation and mapping are essential when providing deliberate instructions that go beyond the on-board sensors’ coverage. The work in (Chen, Wu, Chen, Lin, Cheng, & Wu, 2019) obtains the global localisation of the user from a GPS receiver and the scene is modelled by a set of distances between the user and objects typically appearing on urban environments, i.e. car, motorcycle, electric bicycle, bicycle, and pedestrian. The system in (Lin, Hahn, & Han, 2013) used an inhomogeneous re-sampling process for top-view image warping and morphology filters for obstacle detection. Additionally, a polar edge-blob histogram was used to estimate the safe walking area. User’s motion was estimated by optimising the reprejection error of ground features tracked using the KLT (Kanade-Lucas-Tomasi) tracker. Finally, ISANA (Li et al., 2019) uses a combination of visual odometry, and grid map alignment for solving the localisation problem. Inertial sensors and depth data can be used to segment the floor and compute normal vectors, aiding in localisation (Ruiz-Sarmiento et al., 2014). Visual landmark-based topological localization techniques can also be employed, utilising identifiable landmarks in the environment for navigation (Serrão et al., 2014). There are multiple assistive navigation solutions that tackle the SLAM problem. (Lee & Medioni, 2015) proposed a hybrid user pose and map estimation algorithm that builds a 2D probabilistic occupancy grid map. Additionally, the system utilises the Fast Odometry from Vision (FOVIS) algorithm for incorporating real-time visual odometry. (Bai, Lian, Liu, Wang, & Liu, 2018) presents a  visual  SLAM  module that leverages the  ORB-SLAM2. ORB-SLAM2 runs three  main  parallel  threads: tracking,  local  mapping and  loop closing, and it is used for creating a ‘Virtual-Blind-Road’: a reference path and a base map of the environment created by a sighted person using their system in an offline phase. Finally, in order to guide a blind user, only the tracking thread is engaged for estimating the user’s location.
