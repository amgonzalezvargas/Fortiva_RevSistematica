Robotic System for Eye Contact Training of Children with Autism Jung Hyun Lee’, Hae Yong Kang’, Dong Hwan Kim”, Jong Suk Choi*, Sung-Kee Park* ' Korea Institute of Science and Technology (KIST), Korea, jhni83@kist.re.kr “Korea Institute of Science and Technology (KIST), Korea (Tel: 82-2-958-6724, Fax: 82-2-958-5650) Abstract-- The proposed method is a robotic system for behavior intervention of children with autism who have deficit in development of communication and interaction with others. This robotic system helps the social training of children with autism in that robot reacts as a social being. We aim to improve the eye contact skill of children with autism, one of main indicators of their development. This paper describes our hardware configuration and methods for analyzing a child behavior based on vision and auditory information, and our human-robot interaction (HRI) architecture. At the end, preliminary experimental results with a non-autistic child will be presented. Index Terms-- Autism, Autonomous Robotic System, Eye Contact Training, Human-Robot Interaction (HRI) Architecture. I. INTRODUCTION Children who are diagnosed with autism have problems of social interaction and communication with other people. Their lack of interaction skills and communication skills of early age causes the problem with becoming a member of society through socialization. Therefore, early intervention for children with autism is essential for all their lives [1]. Etiology of autism is unclear [2], and there is no single treatment until now. However, applied behavior analysis (ABA) treatment, one of behavioral intervention, is a scientifically validated approach and has most widely employed. In recent year, autism research using a robot [3] shows new possibilities to learn social behavior. Children with autism have been reported that they often interested in machines or toys which have simple mechanism or form [4], and they even more engrossed in interacting toys than they are not [5]. Thus, we propose a system using a robot for treatment of children with autism. We aim to develop an unsupervised robotic system designed to interact with an autistic child and a robot focusing on eye-contact training, which is the most important skill in social interaction and communication. Before validating the hypothesis that the social interaction skills can be improved when an autistic child interacts with a robot, we implemented a pilot study with a non-autistic child. This paper is organized as follows. In sectionll, we describe a robotic system design including hardware configuration and method. In sectionlll, we describe interaction model for eye contact training. In sectionlV, we describe implementation and summary. II. ROBOT SYSTEM DESIGN Robot-assisted systems have been proposed as clinical tools to provoking social interaction of children with autism [3] [6], but these are designed to have difficulties in operating robot system for a non-robot user or therapies. Some studies showed promising results [7] [8] minimizing these, yet many problems still remain. We proposed a fully autonomous and unsupervised robotic system, which elicits a social behavior for intervention with children with autism. Our approach measuring and analyzing data associated with interaction of a child is designed to react to a child without any other control process. Fig. 1 shows our system configuration and data flow. @ iceiP EEG Signals Response Request Sound Source, Voice Tone Sound Source Localization, TES, Voice Recognition ARCHITECTURE Position, Distance Head and Body veloci Face/ Eyes Detectio Result Detection and Tracking (motion regions), Calculating Distance between each other and Velocity of Movement, Face and Eyes Detection onse Images Response: Task Performance Request (Saying, Dancing, Singing...) Fig. 1. Hardware configuration and data flow. Our system consists of server-client type: robot, vision, audio, Electroencephalogram (EEG) servers, and a Human-robot Interaction (HRI) client. The HRI client requests data from the servers, and each server sends data to HRI client. Then the HRI client analyses the data and command tasks to the robot server. TCP/IP protocol is used for communicating between the servers or the client. A. Robot Server In the robot server, a camera which is placed inside the robot captures images for detecting face of a child, and then the images are transmitted to the vision server. Authorized licensed use limited to: UNIVERSIDAD DEL ROSARIO. Downloaded on December 12,2023 at 22:45:12 UTC from IEEE Xplore. Restrictions apply. Furthermore, the robot performs the scenario tasks received from the HRI client. In other words, the robot interacts with a child by speaking, dancing and so on. B. Vision Server The vision server performs measuring the motion information of a robot or a child with depth images and color images from a 3D video stream in real time. This is mainly composed of detection and tracking parts. To detect motion region, we apply Background Subtraction techniques by using an Adaptive Gaussian Mixture Model (AGMM) [9] [10]. AGMM models background robust to dynamical changes and segregates effectively moving pixels from background model. By using depth images, this detected motion region is robust to illumination variance than color space. Each detected moving regions are classified into a child or a therapist or a robot in predefined shape information and in context of the current scenario. In tracking module, the detected motion regions are tracked by Particle Filter [11] [12]. To estimate a location of moving object, the particle filter needs many samples which predict the next movement of the object. Fig. 2 shows the overall process of detection and tracking module. Moving Blob detection Particle Filtering (AGMM ) | tatetng | Geometry inf. (Environment Inf., Object Size Inf.) Sample Correction Blob Filtering Sample and Target omparison Next position determination Object feature Generation/update Fig. 2. Detection and tracking flow overview Then, we get the distance between the child and the robot, velocity of the child body movement and Head movement from the tracked region. The child’s face direction is an important cue in the eye contact training, which is because it is an indicator to measure the child’s concentration. In the proposed method, we approximate the gaze direction to the face direction. The robot transmits the images from the camera a little above its eyes. Based on these images, our system detects whether the child focuses on the robot’s face or not, when the robot tries to greet the child. Our face detection method is trained with frontal face with our database and detected the frontal face and eyes in it [13]. C. Audio Server and Sound localization The audio server performs sound source localization [14], Text-to-Speech (TTS) and voice recognition. We will use voice tone in the future. D. EEG Server The EEG server is planning to measure EEG signals from a child, but difficulties in wearing an EEG cap on an autistic child still remain, which will be used in the future. E. HRI Client and Architecture The HRI client requests information about behavior states of the child from each server in real time; for example it obtains distance between the child and the robot from vision server. Those collected data about the child information constitute Psychological State Model (PSM) in’ HRI-architecture. Through a_ physical conversion, we convert them into three parameters of PSM for children with autism, for example, pleasure, arousal and intimacy. These three parameters are inputs to our new 3D PSM. Its output value results in selecting which task is selected to the robot, called Social Mode Selection (SMS) process. The output value is affects not only the task selection, but also the motion of robot called Empathy of Robot Motion (ERM) which makes the robot act as a social individual who response to others. The difference between [15] and ours is that our model is based on three-dimensional emotional model of emotion and psychology called attachment theory to express one dedicated shades of psychology. Indicators or data that should be noted about child’s behavior are recorded during implementation. Fig. 3 presents basic structure of our Architecture. Convert INPUT to Parameters (Pleasure, Arousal, Intimacy) Children with Autism ERM (Velocity, Extensity) sacle > INPUT (Behaviors) SMS (Therapy Mode, Encourage Mode, Pause Mode ) PSM (Emotional Density) Social Interaction Fig. 3. Basic structure: the Architecture for Behavioral Therapy of Children with Autism III. INTERACTION MODEL FOR EYE CONTACT TRAINING The basic framework of our robot intervention model, eye contact training, is ABA. In this framework, a child with autism, therapist, and the robot all participate in the training activity. For some cases, only a child with autism and the robot are included. The specific steps are as follows. A. Interaction Model for Eye Contact Training Step1: Operate the robot platform (the robot does not move on the table). Step2: The robot move to designated position on the table. Step3: The therapist and the child sit on the chair in front of the robot. Step4: The robot moves toward the therapist, and then, the robot makes an attempt to greet to the therapist (nodding of its head with saying “Hello?”). The robot, if possible, greets to the therapist with exaggerated speech and act. Step6: The therapist shows to the child that he/she greets the robot gladly. Step7: The robot moves toward the child. Steps: The robot looks at the child and makes an attempt to greet (nodding of its head with saying “Hello?”). Step9: The robot tries to attract the child’s attention saying hello until there is a child’s reaction. If the child shows aversive response, this system pauses. Otherwise, the robot says Authorized licensed use limited to: UNIVERSIDAD DEL ROSARIO. Downloaded on December 12,2023 at 22:45:12 UTC from IEEE Xplore. Restrictions apply. ‘Nice to meet you” to the child, and then, sings a song and dances. If the child does not react to the robot, the robot makes an attempt to greet in other ways. And the robot is disappointed, if the child is indifferent to the robot’s repeated trials. TV. IMPLEMENTATION AND SUMMARY A. Experiment setup This experiment is conducted on a non-autistic male child of 6 years old. There is a table in the middle of the room, and the robot is located at the initial position on the table. At both ends of the table, there are microphones and a camera. A partition is set up between the table and the computer (vision & audio sever, HRI Client) in order to make the child concentrate on this training. The robot is the “IRobiQ” of the Yujin Robot Co. Ltd., and the camera is “XtionPro” of the ASUS. The localization system that the Korea Institute Science Technology (KIST) developed is used. (Fig. 4) iw Fig. 4. Experiment environment at KIST B. Summary Our experimental took 15-frames per second. In the first section, the robot, the child and the therapist are participated together. In the second section, only the robot and the child are participated. Our system worked therapy mode, encourage mode. Sometimes during training, it selected pause mode when the child appeared to be unfamiliar with the robot. V. CONCLUSION We performed this pilot study in order to validate the hypothesis that behavioral intervention using robots provokes interaction and communication skills from children with autism. However, we carried out research on a non-autistic male child in this case. Therefore we cannot evaluate clinical efficacy. In the future, we will experiment on children with autism to test validity of improvement in their eye contact skill. ACKNOWLEDGMENT This research was supported by the KIST Institutional Program. (2E24123) REFERENCES [1] C.M. Corsello. “Early intervention in autism,” /nfants and Young Children, 18, 74—85, 2005 [2] I.S.Baron, “Autism spectrum disorder: complex, controversial, and confounding”, Neuropsychol. Rev. 2008;18:271-2 [3] D. Ricks and M. Colton, "Trends and considerations in robot-assisted autism therapy," 20/0 [EEE International Conference on Robotics and Automation, Anchorage, AK, 3-8 May 2010, in press. [4] B. Robins, K. Dautenhahn, and J. Dubowski, “Does appearance matter in the interaction of children with autism with a humanoid robot?” Jnteraction Studies, 7:3 (2006), 479-512. [5] C.M. Stanton, P.H. Kahn Jr., R.L. Severson, J.H. Ruckert, and B.T. Gill. “Robotic Animals Might Aid in the Social Development of Children with Autism.” Procs. ACM/IEEE Int. Conf. on Human- Robot Interaction, Amsterdam, Netherlands (2008). [6] D. J. Feil-Seifer and M. J. Mataric¢, “Toward socially assistive robotics for augmenting interventions for children with autism spectrum disorders”, In //th International Symposium on Experimental Robotics 2008, Volume 54, Pages 201--210, Athens, Greece, Jul 2008. Springer [7] D. J. Feil-Seifer and M. J. Mataric. “Automated detection and classification of positive vs. negative robot interactions with children with autism using distance-based features.” In Proceedings of the International Conference on Human- Robot Interaction, Lausanne, Switzerland, Mar 2011. [8] H. Kozima, and C. Nakagawa, "Interactive robots as facilitators of children's social development," in Mobile Robots towards New Applications, Aleksandar Lazinica, Ed., Vienna: Advanced Robotic Systems, 2006, pp.271- 286. [9] C. Stauffer and W.E.L. Grimson, "Adaptive Background Mixture Models for Real-Time Tracking", Proc. Computer Vision and Pattern Recognition 1999 (CVPR '99), June, 1999. [10] Z. Zivkovic, "Improved Adaptive Gaussian Mixture Model for Background Subtraction", Proc. Int’l Conf. Pattern Recognition, vol. 2, pp. 28-31, 2004. [11] M. Isard, A. Blake, “CONDENSATION — conditional density propagation for visual tracking”, Int. J. Computer Vision, 29(1):5—28, 1998 [12] S. Arulampalam, S. Maskell, N. Gordon and T. Clapp, “A tutorial on particle filters for on-line nonlinear/non- Gaussian Bayesian tracking’, JEEE Transactions on Signal Processing, 50(2):174-188, 2002 [13] P. Viola and M. Jones "Robust Real-Time Face Detection", Int. J. Computer Vision, vol. 57, no. 2, pp.137 -154 2004 [14] H. Kang and J. Choi, "Sound Source Localization Using Window Function Filtering and Weighted Cumulative Histogram Method," in Information Control Problems in Manufacturing, 2012, pp. 1808-1813 [15] K. Nakagawa, K. Shinozawa, H. Ishiguro, T. Akimoto, and N. Hagita, "Motion modification method to control affective nuances for robots," in Intelligent Robots and Systems, 2009. [ROS 2009. IEEE/RSJ International Conference on, 2009, pp. 5003-5008 Authorized licensed use limited to: UNIVERSIDAD DEL ROSARIO. Downloaded on December 12,2023 at 22:45:12 UTC from IEEE Xplore. Restrictions apply. 