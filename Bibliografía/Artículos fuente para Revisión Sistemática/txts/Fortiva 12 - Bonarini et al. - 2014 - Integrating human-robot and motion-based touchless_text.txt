Integrating Human-Robot and Motion-Based Touchless Interaction for Children with Intellectual Disability Andrea Bonarini, Franca Garzotto, Mirko Gelsomini, Matteo Valoriani Department of Electronics, Information and Bioengineering - Politecnico di Milano Via Ponzio 34/5, 20133, Milano, Italy {andrea.bonarini, franca.garzotto, mirko.gelsomini, matteo.valoriani}@polimi.it ABSTRACT Our research explores the integration of motion-based touchless interaction with human-robots interaction to support game-based learning for children with intellectual disability. The paper discusses the design challenges of this novel approach and presents the design concepts of our initial prototypes. Categories and Subject Descriptors H.5.2 [Information Interfaces and Presentation]: Multimedia Systems, User Interfaces General Terms Design, Experimentation, Human Factors Keywords Human-Robot Interaction, motion-based touchless interaction, full-body interaction, children, disability 1. INTRODUCTION In recent years we have witnessed a rapid growth of learning applications for children with different kinds of disabilities. These tools exploit different learning paradigms and employ a gamut of “beyond the desktop” interaction modes and devices, including haptic controllers, (multi)touch small and large displays [3], digitally augmented physical objects, robots [4] and motion-sensing cameras [1]. Our research explores novel interactive solutions for children with intellectual disability and suffer of significant limitations both in intellectual functioning, i.e., general mental capacity such as memory, attention, reasoning and problem solving, and in adaptive behavior, 1.e., social and practical skills related to daily living (interpersonal relationships. social responsibility, ability to follow rules/obey laws, personal care). Our goal is to provide intellectually disabled children with game-based learning tools that integrate motion-based touchless interaction and interaction with mobile robots. In motion-based touchless interaction, the ingredients are a motion-sensing device (e.g., a Kinect camera) and a virtual world displayed on a medium-large screen that is controlled by means of body movements and gestures without wearing any additional device. This form of interaction supports kinesthetic and visual learning paradigms, and has been proved effective to promote the development of behavioral skills and cognitive functions related to attention, body awareness, awareness of the physical space, meaning construction, and imagination [1][2]. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. Copyright is held by the owner/author(s). AVI ‘14, May 27 — 30, 2014, Como, Italy ACM 978-1-4503-2775-6/14/05. http://dx.doi.org/10.1145/2598153.2600054 341 Human-robot interaction is mainly used with intellectually disabled children to develop competencies in the social sphere. Socially interactive robots create interesting, appealing and meaningful interplay situations that compel children to interact with them, to communicate, to express and perceive emotions, to interpret natural cues and to maintain social relationships. In principle, the integration of these two interaction paradigms in a single learning activity should offer opportunities to achieve all the above mentioned benefits. Still, the coordinated use of motion-based touchless interaction and human-robot interaction in a single learning experience is largely unexplored in the current literature and pedagogical practice, and raises a number of challenges in terms of technology and UX design. The integration of the different hardware and software components related to motion-sensing devices, visual interfaces, and robots requires sophisticated programming solutions. From a UX perspective, the mix of the two paradigms leads to an articulated system of interaction relationships among a triad of actors (robot, child, virtual world): child < robot, child — virtual world, robot — virtual world, and [robot + child] — virtual world. These interaction relationships must be instantiated into fine grained interactions among the robot, the child, and the contents in the virtual world, e.g., the characters of a story or the elements of a virtual game, which take place in the physical space (Fig. 1). The overall picture gets more complex if more than one human actor is involved. In this case, we need to include also social interaction among children (or between a child and her adult caregiver), between them and the virtual word, and between them and the robot. The motion-based touchless interaction child/children/child + adult — virtual world and the child < robot interaction have been explored by prior studies, also in the context of children’s disability. There is a limited knowledge about the other types of interactions — robot — virtual world, [child + robot] — virtual world, [robot+ children/child + adult] — virtual world — and how to orchestrate all these paradigms together. The design challenge is to master the intrinsic complexity of both each individual interaction paradigm and their integration, identifying the roles of the different actors, the interplay situations and _ their orchestration, and to achieve solutions that are appropriate for the special requirements and constraints of intellectually disabled children. 2. DESIGN CONCEPTS Our preliminary prototypes are conceived as simple games for children with severe intellectual disability (e.g., low-medium functioning autistic children). The game experiences have been designed by a multidisciplinary group composed by engineers and designers from our lab, and the specialists (psychologists, motor/psycho-therapists, special educators and neurological doctors) from three therapeutic centers that are currently collaborating with us in the contexts of various projects. Our games involve a single child and are inspired by activities that are frequently proposed to our target group in these centers: well-known physical games for young children, simple recognition tasks, and storytelling. Children’s movements and gestures, as detected by a Kinect camera, affect the behavior of the elements of a virtual world presented on a large screen. The robot acts out as requested by the situation that takes place in the virtual world, moving around inside the play area and giving sound or visual stimuli (e.g., verbally repeating what to do, or moving or highlighting some of its components) that enhance the child’s understanding of the situation on display, promote imitation skills, provide engaging feedbacks to actions and positive reinforcement. To guide the design activity, we have defined different functional roles for the mobile robot and have associated them to the different interaction relationships defined in the previous section. In the Feedback role, the robot acts as a rewarding agent, it reacts to an action performed by the child (robot — child). As Facilitator, the robot suggests what to do and when to do it, facilitating play (robot — child; [robot + child] — virtual world). In the Prompt role, the robot acts as a behavior-eliciting agent enhancing the entire game play (/child + robot] — virtual world). As Emulator, the robot supports the child’s imitative reaction, either acting as the child or exhibing behaviors that must be emulated (child < robot). As Restrictor, the robot identifies the spatial constrains on the child’s movements or offers a set of choice for actions and decisions (robot — child). The robot plays one or more of these roles in the six game prototypes (Figure 1). 1.4) Traffic light 1.5) Witch says color Figure 1: Concepts of initial prototypes. In Hide&Seek, the child and the robot conceal themselves in the environment (using physical objects as hiding places) and must be found by one or more virtual seekers acting the virtual space, by effect of the sensing capability of the Kinect. The robot’s roles are Emulator, Facilitator and Feedback. In Statues, a character in the virtual world acts out as the “Curator” while the robot and child are the “Statues” who must attempt to run towards it and freeze in place when the Curator calls “Stop!” Here the robot acts as Emulator, Facilitator and Feedback. In Colors and Traffic Light, the robot is equipped with some color lighted buttons. In Colors, when a colored figure (e.g., a fruit, an animal) appears on the screen, the child must click the corresponding button on the robot, who acts as Restrictor, Feedback and Facilitator. In Traffic Light, the virtual world, representing a street scene and the robot represents the Traffic Light. The robot can call “Green light!” or “Red Light!” triggering its corresponding button to light up. When the red button is highlighted, cars are moving in the virtual world and the child must stand in place. When the green button is highlighted zebra crossing appears and 1.6) Stories 342 the child can move forward. The robot acts as Facilitator, Restrictor and Feedback. In Witch Says Color, large colored rings are placed on the floor and appear in the virtual world. When a virtual character (avatar) enters a colored circle and says the color name, the robot moves towards the corresponding physical circle and the child imitates it. The robot plays the roles of Emulator, Facilitator, Feedback and Restrictor. In Stories, the child and the robot appears inside the virtual world as story characters, and the robot acts as Mediator, Facilitator and Emulator. For the narrative to continue, the robot and the child must imitate the simple behaviors of virtual characters (e.g., twisting, moving left-right, moving close-away) or perform according to their oral instructions, (e.g., say: “Hello”’). 3. CONCLUSIONS A number of studies show the effectiveness of digital interactive technologies for intellectually disabled children and reveal that they elicit many positive outcomes and responses that normally do not occur using other methods. With few exceptions (e.g., [5]) existing approaches exploit a single interface and interaction paradigm only. Our research attempts to define novel design solution for intellectually disabled children that integrate motion-based touchless interaction with human-robot interaction and with materials in the physical space. We _ have conceptualized the complex structure of interaction relationships involving the various actors (robot, children, and virtual world on display) as well as their mutual roles, and have prototyped a set of simple games that exemplify the interplay of different kinds of interactions and robot — child - virtual world roles. By the end of this year, the effectiveness of our solutions will be tested in a set of field studies in three therapeutic centers. 4. ACKNOWLEDGEMENTS We thanks children and caregivers at Centro Benedetta D’Intino, S. Camillo Hospital, Associazione Astrolabio. This work is supported by the Polisocial Award Program 2013-14. 5. REFERENCES [1] Bartoli L., Corradi C., Garzotto F., Valoriani M. 2013 Motion-based Touchless Interaction for Autistic Children's learning. Proc. Interaction Design and Children (IDC) 2013, ACM, New York, NY, 53-62. [2] Bartoli L., Garzotto F., Gelsomini M. Valoriani M. 2014 Designing and Evaluating Touchless Playful Interaction for ASD Children. Proc. Interaction Design and Children (IDC) 2014, ACM, New York, NY (to appear). [3] Hourcade J.P., Bullock-Rest N., Hansen T.E. 2012 Multitouch Tablet Applications and Activities to Enhance the Social Skills of Children with Autism Spectrum Disorders. Personal and Ubiquitous Computing 16, 2 (Feb 2012), Springer, 157-16 [4] Cabibihan J-J., Javed H., Ang M., Aljunied S. M.. 2013 Why Robots? A Survey on the Roles and Benefits of Social Robots in the Therapy of Children with Autism. Jnt. J. Social Robotics, 5, 4 (Nov. 2013), Springer, 593-618. [5] Zalapa R., Tentor1 M. Movement-based and _ tangible interactions to offer body awareness to children with autism. Ubiquitous Computing and Ambient Intelligence. Context-Awareness and Context-Driven Interaction LNCS 8276, Springer, 127-134. 