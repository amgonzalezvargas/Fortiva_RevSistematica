See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/342561684 ResearchGate Non-Participatory User-Centered Design of Accessible Teacher-Teleoperated Robot and Tablets for Minimally Verbal Autistic Children Conference Paper : June 2020 DOI: 10.1145/3389189.3393738 CITATIONS 0 11 authors, including: Jamy Li University of Twente 37 PUBLICATIONS 856 CITATIONS SEE PROFILE Pauline Chevalier Istituto Italiano di Tecnologia 25 PUBLICATIONS 135 CITATIONS SEE PROFILE Some of the authors of this publication are also working on these related projects: Project SQUIRREL View project Project © SNOOZLE View project All content following this page was uploaded by Bob Schadenberg on 30 June 2020. The user has requested enhancement of the downloaded file. READS 18 Daniel Davison University of Twente 21 PUBLICATIONS 122 CITATIONS SEE PROFILE mY Bob Schadenberg University of Twente 13 PUBLICATIONS 37 CITATIONS SEE PROFILE Non-Participatory User-Centered Design of Accessible Teacher-Teleoperated Robot and Tablets for Minimally Verbal Autistic Children Jamy Li Alyssa Alcorn Snezana Babovic Dimitrijevic Daniel Davison Alria Williams Suncica Petrovic University of Twente University College London Serbian Society of Autism Enschede, The Netherlands London, UK Belgrade, Serbia Pauline Chevalier Bob Schadenberg Eloise Ainger Italian Institute of Technology University of Twente Oxford Health NHS Foundation Trust Genoa, Italy Enschede, The Netherlands Watford, UK Liz Pellicano Macquarie University Sydney, Australia Vanessa Evers University of Twente Enschede, The Netherlands Figure 1: Interaction sequence for a face feature activity that uses an adult’s tablet, a child’s tablet and a humanoid robot. ABSTRACT Autistic children with limited language ability are an important but overlooked community. We develop a teacher-teleoperated ro- bot and tablet system, as well as learning activities, to help teach facial emotions to minimally verbal autistic children. We then con- duct user studies with 31 UK and Serbia minimally verbal autis- tic children to evaluate the system’s accessibility. Results showed minimally verbal autistic children could use the tablet interface to control or respond to a humanoid robot and could understand the face learning activities. We found that a flexible and powerful wizard-of-oz tablet interface respected the needs of the children and their teachers. Our work suggests that a non-participatory, user-centered design process can create a robot and tablet system that is accessible to many autistic children. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. PETRA ’20, June 30-July 3, 2020, Corfu, Greece © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7773-7/20/06. ..$15.00 https://doi.org/10.1145/3389189.3393738 51 CCS CONCEPTS - Human-centered computing — Empirical studies in HCI; Usability testing; Accessibility design and evaluation methods; « So- cial and professional topics — Children. KEYWORDS Human-robot interaction; assistive technology; robot; tablet; mini- mally verbal autistic children; Autism Spectrum Disorder; teleoper- ation interface; limited language; facial expressions ACM Reference Format: Jamy Li, Daniel Davison, Alyssa Alcorn, Alria Williams, Snezana Babovic Dimitrijevic, Suncica Petrovic, Pauline Chevalier, Bob Schadenberg, Eloise Ainger, Liz Pellicano, and Vanessa Evers. 2020. Non-Participatory User- Centered Design of Accessible Teacher-Teleoperated Robot and Tablets for Minimally Verbal Autistic Children. In The 13th PErvasive Technologies Related to Assistive Environments Conference (PETRA ’20), June 30-fuly 3, 2020, Corfu, Greece. ACM, New York, NY, USA, 9 pages. https://doi.org/10. 1145/3389189.3393738 1 INTRODUCTION An emerging goal in human-robot interaction and autism research is to have robots successfully help autistic children! learn about 1We use this term in accordance with preferences of the UK autism community [1]. PETRA ’20, June 30-July 3, 2020, Corfu, Greece emotions. Robot-assisted therapy has advantages such as data col- lection potential and hygiene compared to existing therapies fo- cused on animals and other objects (cf. [2]). Typically, robot tech- nologies are developed to help autistic children who are cognitively able (i.e., use verbal language). Minimally verbal autistic children (i.e., with limited language ability) are often neglected as users of assistive technology, despite the fact that 30% of autistic chil- dren are minimally verbal [3] (cf. [4]) and that this subgroup may need more help than other autistic children [5-8]. Developing a robot and tablet system for minimally verbal autistic children could help designers make similar technologies targeted to this neglected subgroup of the autism community. Robots and tablets are leading technologies being used with autistic children [2, 9-11]. Past user studies explored how autistic children respond to robots and tablets, but have not looked at minimally verbal autistic children’s responses. Given that few if any robots have been developed for minimally verbal autistic children, an evaluation of how this important subgroup of autistic children respond to robot and tablet systems can aid technologists interested in autism and autism researchers interested in technology aids. Robots that play learning activities with autistic children often require either a trained researcher to operate, operate autonomously or operate autonomously with a teacher being able to cancel any undesired actions. However, trained researchers are impractical for everyday use, while autonomous or supervised autonomous robots run the risk of automation errors negatively impacting interactions with autistic children [12], cf. [13]. Giving a teacher the ability to teleoperate the functions of a robot could improve the quality of activities delivered to an autistic child. Most past work that develops robots or other technologies for minimally verbal autistic children evaluates their designs implicitly through the process of participatory design, or explicitly through usability tests with only a few minimally verbal autistic children. Co-design methods and case studies might not be suitable to eval- uate a technology across many autistic children. As participatory co-design is a de facto design method catered to a single or small group of autistic children, yet also potentially challenging for autis- tic children with limited verbal communication [14], it would be interesting to see whether a different user-centered process could produce a robot system suitable for a large number of minimally verbal autistic children. Given that the minimally verbal children in our usability studies did not overtly know that they were being involved in a design process, we do not claim to use participatory design (at least in the traditional sense). We also question the valid- ity and appropriateness of participatory design for individuals who do not understand the context and reason for the research, as was the case with our user group. Nevertheless, the involvement of the autistic children in our research had a significant impact on our pro- cess and design outputs. In the spirit of past participatory methods involving autistic children, our method was nonetheless designed to create “user-focused and user-oriented” outcomes and “elicit feedback...to improve the system” [14]. For example, our focus on evaluating system accessibility rather than child performance was a deliberate methodological choice we made to undertake research with autistic children rather than on them (cf. [15]). Our work contributes: (1) a novel robot and tablet system for face learning activities catered to minimally verbal autistic children; (2) 52 Li et al. user studies with 31 minimally verbal autistic children that assess how they respond to a robot, a tablet and face feature activities; (3) a validated tablet interface for teacher teleoperation of the robot; and (4) empirical evidence that a non-participatory, user-centered process can produce learning technology that accommodates a large number of minimally verbal autistic children. Compared with existing studies on robot therapy for autistic children [16, 17], our work focuses on minimal verbal children, develops a tablet so the teacher and child can lead the robot (i.e., fully-interactive system instead of a system to test children, e.g., [17]), looks at accessibility of a fully-interactive system instead of child performance on a test (e.g., [18]), and is evaluated with a large sample of autistic children (31, some past studies only test with 4 [16] or 11 [18], which limits generalizability of results). 2 BACKGROUND 2.1 Assistive Technology for Minimally Verbal Autistic Children to Learn About Faces About 30% of autistic children have limited or no verbal language (i.e., “minimally verbal” [4, 19]). These children have differences in communication compared to autistic children with verbal language. This subgroup may need technology to be catered to them [7]. Past work has designed smart devices but not robots for mini- mally verbal autistic children, despite the potential robots demon- strate for verbal autistic children [2]. [20] built a tangible ball proto- type to explore joint attention and imitation with minimally verbal autistic children. [21] explored how a tablet training program could improve minimally verbal autistic children’s manual and oral motor skills. [22] built a tablet app that stored photos to help minimally verbal autistic children learn the alphabet and words for household items. [8] constructed a tablet to train minimally verbal autistic children to work with graphics and objects. However, these tech- nologies for minimally verbal individuals did not use a robot. Moreover, the aforementioned work did not explore how face features and emotions can be taught through technology, perhaps because technologies for minimally verbal children tend to focus on trying to improve verbal skill (cf. [20]). One important skill for interpersonal interaction is understanding facial expressions [23]. Autistic children, including those with limited language, often have difficulties in recognizing and processing facial expressions compared to non-autistic children [24, 25]. Past experiments find autistic children show no advantage in recognizing face emotions from seeing one face feature compared to another (e.g., just seeing eyes versus just mouth to identify fear), although which features are seen in piecemeal presentation of faces does matter for typi- cally developing children [26]. Moreover, autistic children show improved recognition with slowed down facial expressions [25]. These studies suggest technology manipulation of faces may be an initial step for face learning programs for autistic children. While it is therefore unsurprising that many robots have been developed to help autistic children learn about facial expressions and emo- tions [16, 18, 27, 28], none have been catered to minimally verbal individuals, who may have difficulty with language. For example, [18] asked autistic children to have a conversation with the Zeno robot (also used in this work) and verbally answer what emotion User-Centered Design of Accessible Robot and Tablets for Minimally Verbal Autistic Children the robot was showing, which might not be possible for those who are minimally verbal. Thus, a robot that can deliver face learning activities catered to minimally verbal autistic children can help broaden pedagogical and interaction possibilities for minimally verbal autistic children. Research Objective 1: Design a robot to address face feature and emotion learning for minimally verbal autistic children. 2.2 Autistic Children’s Responses to Robots and Tablets Past work finds autistic children respond well to a robot. [29] found autistic children spontaneously approach and interact with a minimal creature-like robot, finding pleasure that they share with their caregiver. [30] found autistic children are not afraid of a non- humanoid robot and spontaneously interact with it. [31] found an autistic child managed attention better when interacting with a humanoid robot Nao compared to his regular class behavior. How- ever, these works did not look at minimally verbal autistic children. An evaluation of how they respond to a humanoid robot can better assess the suitability of robots for this group. Research Objective 2a: Observe how accessible a robot activity is for minimally verbal autistic children. Tablets are a leading technology for activities with autistic chil- dren [9]. Past work on tablets or touchscreens finds no strong evidence that the devices are detrimental to autistic children, that autistic children prefer tablets to traditional learning methods, that parents of autistic children report 97% of their children use an iPad 4.6 days per week on average and that 65% of autism profession- als use iPads as rewards or interventions (cf. [32]). (We note that learning outcomes with tablets, rather than the usability results above, are much more varied; for reviews, please see [9, 32].) Only one past study we are aware of explored tablet usability for min- imally verbal autistic children. [33] found three autistic children with limited or no verbal language able to use an iPad as a speech generating device in academic and social situations. However, it remains unknown whether autistic children who are not verbal respond well to a tablet used to interact with a robot, rather than a teacher or a peer as in [33]. An empirical study of this could aid designers of robots for minimally verbal children. Research Objective 2b: Observe how accessible min- imally verbal autistic children find using a tablet to “converse” with a robot. Face learning activities for autistic children primarily teach facial expressions using pictures, movies or tablets (for a review, see [24]). Robots have also been used to teach facial expressions. [27] and [18] asked autistic children to identify the emotion expressed in different robots’ faces. [34] had the Zeno robot used in the current work mirror one autistic child’s face expression; [16] and [28] had robots make expressive faces that children could imitate. However, past face learning activities with either robots or other media have not been designed for minimally verbal children, who may require simpler tasks. As evidence of this, all past work mentioned here use whole face expressions instead of separate face “features” (e.g., just eyes, just eyebrows; for a review of face processing, see [35]). A 53 PETRA ’20, June 30-July 3, 2020, Corfu, Greece face-feature-based learning strategy, which starts out with separate facial features and then moves onto whole facial expressions, could cater for minimally verbal people. Research Objective 2c: Observe how minimally ver- bal autistic children respond to face feature learning activities. 2.3 Robot Teleoperation Interfaces for Teachers Past research explores different levels of autonomy in robots for autistic children. [36] developed a fully autonomous robot and showed autistic children’s social skills could improve after learning with it. [13, 37] designed a “supervised autonomy” robot for learn- ing activities with autistic children, in which the robot’s actions could be approved or rejected by a teacher prior to its execution and additional brief actions such as gaze behaviors could be sent to the robot. One reason to have the teacher supervise the robot’s behavior is that full autonomy will not be perfect and any mistakes it makes may be critical to child interaction [37]. However, these past works did not explore a teleoperated robot, in which a teacher has control over the robot actions, rather than just approving them. A teleoperator interface for teachers may avoid errors in auton- omy and support context-driven robot behavior, which could be important for autistic children with limited verbal language. Research Objective 3: Design a teleoperator tablet for accessible control of a humanoid robot. 2.4 Evaluation Methods for Autism A key challenge in designing technology for autistic children is the generalizability of designs to multiple children. Currently, a leading method of designing robot systems for autism is participa- tory co-design, which creates designs that account for an individual child’s needs [38-41]. However, with participatory methods, it is unclear whether a design from a single child can be extended to other children. Notable efforts to pursue participatory evaluation (i.e., in which the child helps evaluate the technology they designed) has so far only been used to evaluate with one or two other chil- dren [41], so has not yet been shown to have broad applicability. Non-participatory user-centered design (i.e., researchers design the system and test it with users) has created technology systems that are accessible to large groups of people, but it is unclear whether a non-participatory user-centered process can create accessible technologies for minimally verbal autistic children. Research Objective 4: Use a user-centered process to create a robot that is accessible to a large number of minimally verbal autistic children. 2.5 Facial emotion recognition research in HCI has primarily explored how various machine learning techniques can be used to under- stand a person’s emotions based on social science literature [42, 43]. One tension in the state-of-the-art is between universal face emo- tions and ambiguity across individuals (cf. [43]), which may be particularly important for autistic children [44]. State-of-the-art Facial Expression Perception & Recognition PETRA ’20, June 30-July 3, 2020, Corfu, Greece facial emotion recognition systems (e.g., [45]) account for individ- ual differences in how a person expresses face emotion and the situational context of their expression, but may also need to ac- count for tasks and other sources of variation [46]. Facial emotion recognition systems that adapt to individuals have been applied to autistic children [47]. These systems have also been used to classify individuals as autistic or not autistic, by looking at facial attributes such as action units, emotion expressions and valence/arousal [48]. Past literature also suggests automatic recognizers output activation- evaluation (i.e., arousal-valence) measures instead of categorical emotions, which are more manageable for computational systems but less usable by laypeople (cf. [43]). Recent state-of-the-art face emotion recognition systems for autistic children output valence- arousal (e.g., [47]) or a blend [48]. 3 DE-ENIGMA ROBOT AND TABLET FOR FACE LEARNING 3.1 Design Process The authors are 4 autism experts, 2 autism therapists and 4 robot researchers. We held collaborative design sessions over 1.5 years. 3.2 Design We present "DE-ENIGMA", a humanoid robot controlled by an adult with a teleoperator tablet that helps the adult play face activities with an autistic child, who communicates to the robot via a tablet. 3.2.1 Software Architecture. The architecture of the child-robot interaction component of the DE-ENIGMA system (same as [34], except ours adds tablets) features a Dialogue Manager, Behavior Realizer and Agent Control Engine (Figure 2). The dialogue manager responds to user events and generates appropriate behaviors (stored as behavior markup language, BML, representations). The behavior realizer converts the behaviors into platform-specific commands, which it schedules and executes accordingly. Verbal scripts were audio-recorded as mp3 files in Audacity in English and Serbian. The agent control engine then executes the commands on the specific platform (robot or a custom tablet application). This architecture is designed to support fluent, responsive, top-down and bottom-up behavior generation and execution (cf. [34]). Software modules (e.g., Flipper 2.0 [49], ASAPrealizer [50]) are configured to communicate either through ROS or Apollo middleware. The perception systems were not used in this work, but are briefly described here. Microphones, RGB cameras and depth cam- eras sense the child’s actions, which are interpreted by perception algorithms as emotional valence, arousal, presence of laughter, etc. 3.2.2. Activities. We developed a set of four activities (Table 1). The teaching strategy focused on designing activities that relied less on verbal language and focused on facial features rather than full faces. These attributes were selected based on previous usability studies [34], in particular the finding that some children in our target user group experienced challenges with language and full faces. The initial script design session resulted in four distinct game activity steps that focused on teaching individual facial features, how facial features could be composed into emotional facial ex- pressions and discrimination of emotional facial expressions. Game scripts (i.e., spoken lines and actions) were subsequently written 54 Li et al. Figure 2: System architecture. All emojis designed by Open- Moji - the open-source emoji and icon project. License: CC BY-SA 4.0 for the dialogue between therapist, child and robot according to existing dialogue composition strategies [34]. For example, based on the strategies of simple language & key phrases, we used short phrasing like “Look and listen” already used in the schools; based on scaffolding, we began with a simple face feature activity before moving to more difficult face emotions. Giving autistic children greater control over the behaviors of a robot could be more engaging and motivating for them compared to passive listening, and therefore facilitate better learning. Supporting this idea, autistic children spontaneously request certain robot behaviours and sometimes attempt to press the teacher’s buttons to control the robot or touch/manipulate the robot directly [51, 52]. Providing children with more input and control over the interaction could be achieved by giving children a tablet interface, which might also help with the desire of children, particularly those with limited expressive language, to touch the robot by redirecting them to touch a tablet. We therefore included an activity step to support children’s exploration and control of the robot (Steps 1 and 3) prior to testing children’s knowledge (Steps 2 and 4). 3.3 Apparatus The robot is Robokind’s 56-cm-tall male cartoon R25 robot “Zeno”, which has 5 degrees of freedom in its face, 2 in its pupils for gaze, 3 in its neck and 6 in its body (we used arm and torso movement only) (Figure 1). The face of the robot is made of synthetic skin. The child and adult teleoperator tablets are Samsung Galaxy Tabs S2 (9.7 inch) with Android 6.0. A tablet case was used over the child’s tablet to prevent pressing the home button. The adult tablet displayed buttons that could be used to directly trigger robot behaviours and initiate game activities (Figure 3). The child tablet User-Centered Design of Accessible Robot and Tablets for Minimally Verbal Autistic Children PETRA ’20, June 30-July 3, 2020, Corfu, Greece Table 1: Activities for face learning Step Color Activity Sample Script Lead 1 Pink Teach face features Adult: “[Adult]’s turn.” [adult touches tablet] Robot: “These are my eyes” Adult Adult: “[Child]’s turn.” [child touches tablet] Robot: “This is my mouth” Child 2 Blue Test face features Robot: “Find my eyes.” [child touches tablet] Robot: [repeats if wrong, cheers if right] Robot 3 Green Teach face emotion Adult: “[Adult]’s turn.’ [adult touches tablet] Robot: “Happy. My mouth is up.I am smiling” Adult Adult: “[Child]’s turn.” [child touches tablet] Robot: “Happy. My eyebrows are up.’ Child 4 Yellow Testface emotion Robot: “Find my happy face.’ [child touches tablet] Robot: [repeats if wrong, cheers if right] Robot displayed learning content and let the child answer the robot’s questions or select actions for the robot to do (Figure 3). 4 USER STUDY We evaluated how minimally verbal autistic children responded to the robot and tablet activities in a UK and Serbia school study. 4.1 Method and Materials 4.1.1 Participants. 31 autistic children (8 girls) aged 4 to 14 (M = 12 yr, SD = 2 yr 1 mo) with Autism Diagnostic Observation Schedule (ADOS) [53] scores between M1 and M2/M3 (M = 1.5, SD = 0.6) participated in user tests in UK and Serbia over a 9-month period between April and Dec 2018. One round of tests was run in Serbia with 11 children (2 girls, M = 9 yr 4 mo, SD = 2 yr 2 mo, ADOS not used in Serbia), while two rounds were held in UK, 1st round with 8 children (2 girls, M = 9 yr, SD = 1 yr 1 mo, ADOS module M = 1.3, SD = 0.5), 2nd round with 16 children (4 girls, M = 9 yr, SD = 2 yr 4 mo, ADOS module M = 1.6, SD = 0.7, therapist-assigned language score M = 2.8 out of 4, SD = 1.1). (Note 4 UK children in the 2nd round participated in the 1st round.) 4.1.2 Procedure. After attaining parent/caregiver assent, children were scheduled for 1-5 sessions of up to 25 minutes over a 3-week period. Each session involved a triadic interaction between the child, the adult and the robot. The child sat in front of a desk on which the robot was placed facing them and a tablet was placed between the child and robot (Figure 4). The adult teacher sat next to the child either with the teleoperator tablet beside them or with a second adult in a separate room using that tablet. Sessions also optionally included free-play with tangible blocks, puzzles or other objects, at the therapist’s discretion. Serbia sessions were held in an autism center while UK sessions were held in a school. Sessions were audio- and video-recorded with assent. All procedures were approved by a UK and Serbia ethics committee. 4.1.3 Analysis. We reviewed all sessions with 1-2 researchers to make observations categorized by child’s response to the robot, tablet, activity and miscellaneous. For each round, observations in each category were sorted and merged into themes for that cat- egory. To compile data across rounds, we looked at themes and observations in each category across all three rounds, in order to synthesize into the final themes presented below. For the teleoper- ator tablet (which was not used by children), we self-reflected on its usability. Observations were catered as much as possible to our experience with each child (e.g., to interpret his or her responses). 55 4.2 Results 4.2.1. Child Response to the Robot. All participating children en- gaged with Zeno and with the tablet content at some point, with many highly engaged throughout. Children were very individual in what aspects they engaged with, and for how long, though over- all they paid more attention to what Zeno was doing than what he was saying. Some children’s attention to the robot and to the activities differed (e.g., high interest to interact with the robot but lower interest in the activity used to interact with it). One Serbia child was afraid of the robot. Overall, the robot was accessible to most children. Children appeared to enjoy sensory-based (i.e., nonverbal actions like the robot dancing, making faces or cheering) actions by the robot. For example, a UK child enjoyed and repeatedly requested a Twinkle Twinkle song and dance by the robot. Several children with limited language (e.g., at least four in the UK 2nd round) appeared to treat the non-verbal sensory actions of the robot as their main source of interest and means of interaction. The ability of the robot to make context-based actions separate from the activity flow (i.e., “Yes”, “No”, “I don’t know’, “Thank you’, [Praise move] and “Wow”, bottom buttons of teleoperator tablet in Figure 3) was valuable for children who attempted to converse with the robot (e.g., a UK child asked the robot questions). 4.2.2 Child Response to the Tablet. Serbia children responded well to the tablet. They appeared familiar with touching it but were not so distracted by it that they didn’t attend to the adult or robot. One child did not want to use the tablet, maybe because he knew it would make the robot move. UK children responded well to the tablet. All children except one seemed to find the tablet accessible for their use. Some children expected immediate reactions after they touched the tablet, and were impatient when they did not occur immediately. For one child, the tablet itself was so distracting that they did not engage with the content; however, most other children were not as distracted by it. One child mixed up the image of the robot with the robot itself: this child attempted to point at the robot’s face on the robot itself in response to its question, rather than touching the tablet (although this was resolved in the child’s third and subsequent sessions). Overall, the tablet was accessible to most children. Children pressed the tablet only enough to select their option. This suggests the feedback system we used, which removes the green outline around face features and darkens it so the face is consistently dark (Figure 3), appeared to be a clear signal to children that their choice was registered (although some younger children PETRA ’20, June 30-July 3, 2020, Corfu, Greece Li et al. Figure 3: From left: screenshots from child tablet during pink, blue, green and yellow games; screenshot of adult (teleoperator) tablet with high- and low-level action buttons. ‘Caregiver™: Z\ camera Figure 4: Study setup. Either the teacher (adult 1) or another adult (adult 2) controlled the robot using the teleoperator adult tablet. Caregiver may be absent. continued to press). Children also spontaneously directed their attention to the robot when the buttons disappeared. 4.2.3 Child Response to Face Feature Activities. In the UK, most children eventually understood all the activities, as demonstrated by their responses and participation. One child did not. Similarly, most children in Serbia were guided quickly through each activity. They participated in the activities by selecting options on the tablet and looking at the robot. These children were able to understand when specific face features (e.g., the mouth) were referred to. Although the activities were accessible to children, they did not seem to be very fun or engaging, as we observed children behaving disinterested or passive. (An exception was the robot dancing or cheering, which was a contextual action and not directly part of the activity.) An increased game or reward aspect may help. Color coding of games (in the child’s tablet screen, robot’s speech and adult researcher’s script) was effective in allowing children to identify what game they wanted to play. Some children asked for games of other colors (e.g., a UK child asked for a Black game). This color coding appeared to correctly avoided issues with children incorrectly thinking that a “teach” activity was being played when they saw a similar tablet screen during a “test” activity (left two screens of Figure 3), which we observed in an initial prototype. 56 4.2.4 Adult Teleoperator Tablet. We reflect on the usability of the teleoperator tablet based on our own experience. Having a separate teleoperator worked well in managing cognitive load for the adult running the session with the child. It required about half a day of training between the teleoperator and the adult teacher. The teleoperator design, with buttons for activity-level execution at the top and quick one-off response buttons at the bottom (Table 3), allowed the operator to issue quick robot actions based on the situation while directing the overall activity flow. 4.2.5 Cross-Cultural Effects. Overall, we observed children in the UK and Serbia to respond similarly (for example, both groups had accessibility issues with the pacing and lack of tablet feedback, which we subsequently resolved for the second round of UK tests). The primary differences that we noticed were in the interactions between robot, therapist and child across regions. Serbian therapists sometimes used the robot sparingly and instead delivered content using their own teaching and switched tasks quickly as compared to the UK therapists; these actions appeared to motivate children. Serbian therapists were also very skilled at directing child attention to the robot, tablet or another location, whereas the UK therapists focused more on evaluating the child’s spontaneous or “natural” response to the robot. 5 DISCUSSION 5.1 Summary of Results We developed a novel robot and tablet system for face feature ac- tivities that was catered to minimally verbal autistic children. User studies with 31 minimally verbal autistic children found that the robot and tablet system was accessible, particularly the robot’s non- verbal actions, the tablet’s button feedback, the activity’s color cod- ing and the teleoperator tablet’s inclusion of both activity-related and context-based buttons. We find a user-centered process can result in a robot system accessible to a group of minimally verbal autistic children. 5.2 Design Implications Table 2 presents preliminary design guidelines for tablets and robots for minimally verbal autistic children based on this work. The main design implication is that we successfully designed tablets, User-Centered Design of Accessible Robot and Tablets for Minimally Verbal Autistic Children PETRA ’20, June 30-July 3, 2020, Corfu, Greece Table 2: Summary of Results and Design Implications Result Robot Children engaged with robot Children liked dance/cheer Some children converse with robot Child Tablet Tablet is accessible Children touched buttons only once Face Activity Activities understood Activities not fun Colors understood Teleoperator Tablet Teleop. tablet usable Separate operator worked Can use robot robots and face feature activities that were accessible to the autistic children (and their teachers) in our study. 5.3. Methodological Implications We show that a non-participatory user-centered process can demon- strate a robot system’s accessibility across a large (N = 31) group of minimally verbal autistic children. Although participatory design methods are valuable to involve autistic children in creating sys- tems they would use, user-centered evaluation may have value as a means to show general accessibility of a system. 5.4 Limitations We do not report a user study evaluating whether our system im- proves learning outcomes for autistic children, as our focus was on evaluating system accessibility rather than evaluating child perfor- mance. We do not report quantitative analyses of child behavior as might be produced by an annotated, time-sequence analysis of video recordings. As our primary goal was to develop an accessible robot and tablet system for minimally verbal autistic children, we tried to identify a comprehensive range of recurring usability issues and excluded quantitative analyses from this study. We leave it as potential future work. We did not have a baseline condition where the therapy was done without a robot and tablet. Instead, we relied on the thera- pists’ knowledge of individual children to interpret their behavior as interest, confusion, understanding or other state. Nevertheless, we tried to support therapists to obtain in-depth knowledge of indi- vidual child differences by including free-play (without the robot) time within sessions and by having each child do multiple sessions over a 3-week period in our study protocol. Our results listed above are based on therapists’ consideration of their holistic experience of each child, including during free-play sessions. We did not use a participatory design process in the traditional sense, since the children involved were likely unaware of their participation in a design process and did not explicitly give design suggestions. We questioned, but did not explore, the appropriate- ness of involving children in co-design or participatory processes Design Implication o/ Include sensory, nonverbal robot actions Allow contextual responses by robot operator Can use tablet for child to control and respond to robot Include tablet button feedback, e.g. button removal, fade into background image Can use face features in activity Add game or rewards to activity Give each activity a color Include both activity-level and quick-response buttons on adult’s tablet Can reduce therapist load with separate operator for adult tablet that they may not fully understand. Despite using a design process that does not qualify as traditional participatory design, it is the children’s involvement and responses that are important in our research and make up the bulk of our results. We did not explore game concepts such as rewards or difficulty levels in creating the activities, which could have made them more fun and more like a game. 6 CONCLUSION We designed a robot and tablet system for face learning targeted to minimally verbal autistic children. In an evaluation study with 31 autistic children in the UK and Serbia, children made choices on the tablet and then directed their attention to the robot, indicating they understood the interaction flow. We demonstrate a robot and tablet system that is accessible for a large number of minimally verbal autistic children. ACKNOWLEDGMENTS Funded by the European Union Horizon 2020 grant No. 688835. REFERENCES [1] Lorcan Kenny, Caroline Hattersley, Bonnie Molins, Carole Buckley, Carol Povey, and Elizabeth Pellicano. Which terms should be used to describe autism? per- spectives from the uk autism community. Autism, 20(4):442—462, 2016. [2] Brian Scassellati, Henny Admoni, and Maja Mataric. Robots for use in autism research. Annual review of biomedical engineering, 14:275-294, 2012. [3] Ericka L Wodka, Pamela Mathy, and Luther Kalb. Predictors of phrase and fluent speech in children with autism and severe language delay. Pediatrics, 131(4):e1128-e1134, 2013. [4] Helen Tager-Flusberg and Connie Kasari. Minimally verbal school-aged children with autism spectrum disorder: The neglected end of the spectrum. Autism research, 6(6):468-478, 2013. [5] Romuald Blanc, Marie Gomot, Maria Pilar Gattegno, C Barthélémy, and JL Adrien. Les troubles de l’activité symbolique chez des enfants autistes, dysphasiques et retardés mentaux et l’effet de l’étayage de l’adulte. Revue québécoise de psychologie, 23(2):23-45, 2002. [6] Helen Tager-Flusberg and Elizabeth Caronna. Language disorders: autism and other pervasive developmental disorders. Pediatric Clinics of North America, 54(3):469-481, 2007. [7] Audrey Duquette, Francois Michaud, and Henri Mercier. Exploring the use of a mobile robot as an imitation agent with children with low-functioning autism. Autonomous Robots, 24(2):147-157, 2008. PETRA ’20, June 30-July 3, 2020, Corfu, Greece [8] — \O = [10] [11] [12] [13] [14] [15] [16] [17 — [18] [19] [20] [21] [22] [23] [24] [25] Maria Vélez-Coto, Maria José Rodriguez-Fortiz, Maria Luisa Rodriguez- Almendros, Marcelino Cabrera-Cuevas, Carlos Rodriguez-Dominguez, Tomas Ruiz-Lépez, Angeles Burgos-Pulido, Inmaculada Garrido-Jiménez, and Juan Martos-Pérez. Sigueme: Technology-based intervention for low-functioning autism to train skills to work with visual signifiers and concepts. Research in developmental disabilities, 64:25-36, 2017. Elizabeth R Lorah, Ashley Parnell, Peggy Schaefer Whitby, and Donald Hantula. A systematic review of tablet computers and portable media players as speech generating devices for individuals with autism spectrum disorder. Journal of Autism and Developmental Disorders, 45(12):3792—3804, 2015. Sara Ali, Faisal Mehmood, Yasar Ayaz, Muhammad Jawad Khan, Haleema Sadia, and Raheel Nawaz. Comparing the effectiveness of different reinforcement stimuli in a robotic therapy for children with asd. IEEE Access, 8:13128-13137, 2020. Sara Ali, Faisal Mehmood, Darren Dancey, Yasar Ayaz, Muhammad Jawad Khan, Noman Naseer, Rita De Cassia Amadeu, Haleema Sadia, and Raheel Nawaz. An adaptive multi-robot therapy for improving joint attention and imitation of asd children. IEEE Access, 7:81808-81825, 2019. Mark Coeckelbergh, Cristina Pop, Ramona Simut, Andreea Peca, Sebastian Pintea, Daniel David, and Bram Vanderborght. A survey of expectations about the role of robots in robot-assisted therapy for children with asd: Ethical acceptability, trust, sociability, appearance, and attachment. Science and engineering ethics, 22(1):47-65, 2016. Pablo G Esteban, Paul Baxter, Tony Belpaeme, Erik Billing, Haibin Cai, Hoang- Long Cao, Mark Coeckelbergh, Cristina Costescu, Daniel David, Albert De Beir, et al. How to build a supervised autonomous system for robot-enhanced therapy for children with autism spectrum disorder. Paladyn, Journal of Behavioral Robotics, 8(1):18-38, 2017. Christopher Frauenberger, Judith Good, Alyssa Alcorn, and Helen Pain. Support- ing the design contributions of children with autism spectrum conditions. In Proceedings of the 11th International Conference on Interaction Design and Children, pages 134-143, 2012. Nick Chown, Jackie Robinson, Luke Beardon, Jillian Downing, Liz Hughes, Julia Leatherland, Katrina Fox, Laura Hickman, and Duncan MacGregor. Improving research about us, with us: a draft framework for inclusive autism research. Disability & Society, 32(5):720—734, 2017. Giovanni Pioggia, ML Sica, Marcello Ferro, Roberta Igliozzi, Filippo Muratori, Arti Ahluwalia, and Danilo De Rossi. Human-robot interaction in autism: Face, an android-based social therapy. In RO-MAN 2007-The 16th IEEE International Symposium on Robot and Human Interactive Communication, pages 605-612. IEEE, 2007. Farzaneh Askari, Haunghao Feng, Timothy D Sweeny, and Mohammad H Mahoor. A pilot study on facial expression recognition ability of autistic children using ryan, a rear-projected humanoid robot. In 2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pages 790-795. IEEE, 2018. Michelle J Salvador, Sophia Silver, and Mohammad H Mahoor. An emotion recognition comparative study of autistic and typically-developing children using the zeno robot. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 6128-6133. IEEE, 2015. Connie Kasari, Nancy Brady, Catherine Lord, and Helen Tager-Flusberg. As- sessing the minimally verbal school-aged child with autism spectrum disorder. Autism Research, 6(6):479-493, 2013. Cara Wilson, Margot Brereton, Bernd Ploderer, and Laurianne Sitbon. Co-design beyond words: moments of interaction’ with minimally-verbal children on the autism spectrum. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pages 1-15, 2019. Emma J Weisblatt, Caroline S Langensiepen, Beverley Cook, Claudia Dias, K Plaisted Grant, Manuj Dhariwal, Maggie S Fairclough, Susan E Friend, Amy E Malone, Bela Varga-Elmiyeh, et al. A tablet computer-assisted motor and language skills training program to promote communication development in children with autism: Development and pilot study. International Journal of Human—Computer Interaction, 35(8):643-665, 2019. Cara Wilson, Margot Brereton, Bernd Ploderer, and Laurianne Sitbon. Myword: Enhancing engagement, interaction and self-expression with minimally-verbal children on the autism spectrum through a personal audio-visual dictionary. In Proceedings of the 17th ACM Conference on Interaction Design and Children, pages 106-118, 2018. RJR Blair. Facial expressions, their communicatory functions and neuro-cognitive substrates. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 358(1431):561-572, 2003. Miriam Madsen, Rana El Kaliouby, Matthew Goodwin, and Rosalind Picard. Technology for just-in-time in-situ learning of facial affect for persons diagnosed with an autism spectrum disorder. In Proceedings of the 10th international ACM SIGACCESS conference on Computers and accessibility, pages 19-26. ACM, 2008. Carole Tardif, France Lainé, Mélissa Rodriguez, and Bruno Gepner. Slowing down presentation of facial movements and vocal sounds enhances facial expression recognition and induces facial-vocal imitation in children with autism. Journal 58 [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] Li et al. of Autism and Developmental Disorders, 37(8):1469—-1484, 2007. Simon Wallace, Michael Coleman, and Anthony Bailey. An investigation of basic facial expression recognition in autism spectrum disorders. Cognition and Emotion, 22(7):1353-1380, 2008. Cristina Anamaria Pop, Ramona Simut, Sebastian Pintea, Jelle Saldien, Alina Rusu, Daniel David, Johan Vanderfaeillie, Dirk Lefeber, and Bram Vanderborght. Can the social robot probo help children with autism to identify situation-based emotions? a series of single case experiments. International Journal of Humanoid Robotics, 10(03):1350025, 2013. Ben Robins, Kerstin Dautenhahn, and Paul Dickerson. From isolation to com- munication: a case study evaluation of robot assisted play for children with autism with a minimally expressive humanoid robot. In 2009 Second International Conferences on Advances in Computer-Human Interactions, pages 205-211. IEEE, 2009. Hideki Kozima, Cocoro Nakagawa, and Yuriko Yasuda. Children—robot inter- action: a pilot study in autism therapy. Progress in brain research, 164:385—400, 2007. Kerstin Dautenhahn and Aude Billard. Games children with autism can play with robota, a humanoid robotic doll. In Universal access and assistive technology, pages 179-190. Springer, 2002. Syamimi Shamsuddin, Hanafiah Yussof, Luthffi Idzhar Ismail, Salina Mohamed, Fazah Akhtar Hanapiah, and Nur Ismarrubie Zahari. Initial response in hri-a case study on evaluation of child with autism spectrum disorders interacting with a humanoid robot nao. Procedia Engineering, 41:1448-1455, 2012. Melissa L Allen, Calum Hartley, and Kate Cain. ipads and the use of “apps” by children with autism spectrum disorder: do they promote learning? Frontiers in psychology, 7:1305, 2016. Joy F Xin and Deborah A Leonard. Using ipads to teach communication skills of students with autism. Journal of autism and developmental disorders, 45(12):4154- 4164, 2015. Pauline Chevalier, Jamy J Li, Eloise Ainger, Alyssa M Alcorn, Snezana Babovic, Vicky Charisi, Suncica Petrovic, Bob R Schadenberg, Elizabeth Pellicano, and Vanessa Evers. Dialogue design for a robot-based face-mirroring game to engage autistic children with emotional expressions. In International Conference on Social Robotics, pages 546-555. Springer, 2017. Sarah Weigelt, Kami Koldewyn, and Nancy Kanwisher. Face identity recognition in autism spectrum disorders: a review of behavioral studies. Neuroscience & Biobehavioral Reviews, 36(3):1060—1084, 2012. Brian Scassellati, Laura Boccanfuso, Chien-Ming Huang, Marilena Mademtzi, Meiying Qin, Nicole Salomons, Pamela Ventola, and Frederick Shic. Improving social skills in children with asd using a long-term, in-home social robot. Science Robotics, 3(21):eaat7544, 2018. Hoang Long Cao, Pablo Esteban, Madeleine Bartlett, Paul Edward Baxter, Tony Belpaeme, Erik Billing, Haibin Cai, Mark Coeckelbergh, Cristina Costescu, Daniel David, et al. Robot-enhanced therapy: development and validation of a supervised autonomous robotic system for autism spectrum disorders therapy. IEEE Robotics and Automation Magazine, 2019. Laura Benton, Hilary Johnson, Emma Ashwin, Mark Brosnan, and Beate Grawe- meyer. Developing ideas: supporting children with autism within a participatory design team. In Proceedings of the SIGCHI conference on Human factors in com- puting systems, pages 2599-2608. ACM, 2012. Christopher Frauenberger, Julia Makhaeva, and Katharina Spiel. Designing smart objects with autistic children: Four design exposes. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages 130-139. ACM, 2016. Christopher Frauenberger, Judith Good, and Wendy Keay-Bright. Designing technology for children with special needs: bridging perspectives through partic- ipatory design. CoDesign, 7(1):1-28, 2011. Katharina Spiel, Laura Malinverni, Judith Good, and Christopher Frauenberger. Participatory evaluation with autistic children. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pages 5755-5766. ACM, 2017. Caifeng Shan, Shaogang Gong, and Peter W McOwan. Facial expression recogni- tion based on local binary patterns: A comprehensive study. Image and vision Computing, 27(6):803-816, 2009. Roddy Cowie, Ellen Douglas-Cowie, Nicolas Tsapatsoulis, George Votsis, Stefanos Kollias, Winfried Fellenz, and John G Taylor. Emotion recognition in human- computer interaction. IEEE Signal processing magazine, 18(1):32—80, 2001. Madeline B Harms, Alex Martin, and Gregory L Wallace. Facial emotion recog- nition in autism spectrum disorders: a review of behavioral and neuroimaging studies. Neuropsychology review, 20(3):290-322, 2010. Nikolaos Doulamis. An adaptable emotionally rich pervasive computing system. In 2006 14th European Signal Processing Conference, pages 1-5. IEEE, 2006. Ognjen Oggi Rudovic. Machine learning for affective computing and its applica- tions to automated measurement of human facial affect. In 2016 International Symposium on Micro-NanoMechatronics and Human Science (MHS), pages 1-1. IEEE, 2016. Ognjen Rudovic, Jaeryoung Lee, Miles Dai, Bjorn Schuller, and Rosalind W Picard. Personalized machine learning for robot perception of affect and engagement in User-Centered Design of Accessible Robot and Tablets for Minimally Verbal Autistic Children [48] autism therapy. Science Robotics, 3(19):eaa06760, 2018. Beibin Li, Sachin Mehta, Deepali Aneja, Claire Foster, Pamela Ventola, Frederick Shic, and Linda Shapiro. A facial affect analysis system for autism spectrum disorder. In 2019 IEEE International Conference on Image Processing (ICIP), pages 4549-4553. IEEE, 2019. [49] Jelte van Waterschoot, Merijn Bruijnes, Jan Flokstra, Dennis Reidsma, Daniel [50] Davison, Mariét Theune, and Dirk Heylen. Flipper 2.0: a pragmatic dialogue engine for embodied conversational agents. In Proceedings of the 18th International Conference on Intelligent Virtual Agents, pages 43-50. ACM, 2018. Dennis Reidsma and Herwin van Welbergen. Asaprealizer in practice—a modular and extensible architecture for a bml realizer. Entertainment computing, 4(3):157- 169, 2013. 59 [51] [52] [53] PETRA ’20, June 30-July 3, 2020, Corfu, Greece Bob R Schadenberg, Dennis Reidsma, Dirk KJ Heylen, and Vanessa Evers. Differ- ences in spontaneous interactions of autistic children in an interaction with an adult and humanoid robot. Frontiers in robotics and AI, 7:28, 2020. Adriana Tapus, Andreea Peca, Amir Aly, Cristina Pop, Lavinia Jisa, Sebastian Pin- tea, Alina S Rusu, and Daniel O David. Children with autism social engagement in interaction with nao, an imitative robot: A series of single case experiments. Interaction studies, 13(3):315—-347, 2012. Catherine Lord, Susan Risi, Linda Lambrecht, Edwin H Cook, Bennett L Lev- enthal, Pamela C DiLavore, Andrew Pickles, and Michael Rutter. The autism diagnostic observation schedule—generic: A standard measure of social and com- munication deficits associated with the spectrum of autism. Journal of autism and developmental disorders, 30(3):205—223, 2000. 