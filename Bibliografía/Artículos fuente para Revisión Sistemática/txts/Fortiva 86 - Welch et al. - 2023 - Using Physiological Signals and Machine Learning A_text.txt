Authorized licensed use limited to: UNIVERSIDAD DEL ROSARIO! Using Physiological Signals and Machine Learning Algorithms to Measure Attentiveness During Robot-Assisted Social Skills Intervention: A Case Study of Two Children with Autism Spectrum Disorder Karla Conn Welch, Robert Pennington, Saipruthvi Vanaparthy, Ha Manh Do, Rohit Narayanan, Dan Popa, Gregory Barnes, and Grace Kuravackel ndividuals with autism spectrum disorder (ASD) often face barriers in accessing opportunities across a range of educational, employment, and social contexts. One of these barriers is the development of effective communication skills sufficient for navigating the social demands of every- day environments. Fortunately, researchers have established evidence-based practices (EBP) for teaching critical commu- nication skills to individuals with ASD [1]. One EBP that has received a great deal of attention over the last few decades is technology-aided instruction and intervention (TAII) [1],[2]. TAII is an instructional practice in which technology is an es- sential component and is used to facilitate behavior change. Further, it encompasses a wide range of applications including computer-assisted instruction, virtual and augmented reality, augmentative and alternative communication, and robot-as- sisted intervention [2]. Over the last two decades, there has been increased inter- est in the application of robot-assisted interventions (RAJ) for teaching social skills to learners with ASD [3]. RAI involves the use of robots to deliver, augment, or support intervention practices. Researchers have employed robots to teach a myr- iad of social skills including joint attention [4], text messaging [5], interpreting and making gestures [4], and emotion recog- nition [6]. During RAI, robots can serve as direct instructional agents, emitting directives or modeling targeted behaviors. For example, Pennington and colleagues used an autonomous, programmable humanoid (NAO) robot to verbally direct stu- dents how to generate text messages [5]. The robot provided a directive to the participant, waited for the participant to in- dicate they were finished prior to emitting the next directive, and then offered to perform a dance at the end of the session. Similarly, another study used an NAO robot to assist in facil- itating game scenarios during pivotal response training [7]. During sessions, therapists controlled the robot to present stimuli, prompt participants to respond, and provide rein- forcing feedback. The application of robots as change agents during social skills intervention may offer several benefits in- cluding increasing the reinforcing properties of intervention packages, limiting human errors in implementation fidelity, May 2023 and reducing social requirements of interpreting subtle social cues emitted by the human instructors (e.g., facial expression, changes in intonation). The majority of investigations in which a robot served as an instructor involved the programming of one-way (e.g., ro- bot presents a directive and the child responds) and two-way (e.g., robot presents a vocal directive, the child responds, the robot detects response and provides feedback or emits next di- rective) interactions, or a Wizard of Oz approach in which the instructor directs the actions of the robot. These approaches are limited in that they rely on an external human change agent to observe, detect, and respond to subtle changes in the learner’s attention; attention is essential to skill acquisition and may be difficult for some learners with ASD. This reliance on an ad- ditional human change agent to facilitate attention during robot-learner interactions ultimately serves as a barrier to the autonomous application of robots to support student learning. A way to address this issue is by incorporating software to detect and respond to physiological correlates for atten- tion within RAI packages. Physiological signals can be an especially useful communication link for children with ASD, whose outward expressions of affect may not be as apparent as developmentally typical children [8]. However, the im- plicit physiological signals of children with ASD do indicate emotional changes to stimuli, including during social inter- actions [8]. Researchers in affective computing have reported physio- logical signals to be a reliable source of objective information related to users’ emotional reactions, because of the signals’ connections to the autonomic nervous system [8]—[12]. How- ever, conducting affective computing research presents challenges in data collection. For example, although com- puter vision could be used to process video streams of users and their reactions [13], this signal presents privacy issues and poses a challenge of collecting video from certain (e.g., prefer- ably head-on) angles at all times [13],[14], whereas privacy is less of a concern with wrist-worn devices [15]. Additionally, some physiological signals are more cumbersome to collect than others and lead to less adoption rates by users [16], such IEEE Instrumentation & Measurement Magazine 39 4-6969/23/$75,000 ownloa on 2023IEEE ecember 11,2023 at 23:28:18 UTC from IEEE Xplore. Restrictions apply. as electroencephalogram which can involve skull caps and conductive gel on a user’s head or respiratory data collected from a strap wrapped around a user’s upper chest. Some sig- nals may be slow to change as compared to others, such as respiratory signals compared to heart rate variability, and therefore limit a closed-looped system’s ability to respond quickly to a user’s change in emotion [14]. Although these signals may provide informative data that can be related to emotions [11],[14], their data collection techniques may limit the type of natural user interactions that can be studied [15] and may preclude the adoption of these sensors in a user’s daily life [13],[16], thus limiting the future capacity of affective computing to be applied to everyday life. Therefore, ambula- tory measurement from sensors with a high adoption rate by users is preferable [8],[17]. Collecting physiological signals from a wristband device addresses many of the data collection challenges in affective computing research. Empatica’s E4 is a commercially available wearable device with embedded sensors that was developed by affective com- puting researchers [18] and has been used in several studies to examine affect and physiological signals [11],[19]. The E4 is one example of how much physiological-sensing equipment has evolved in the last decade [8],[16] and matches the char- acteristics of an appropriate wearable sensor suggested for use with the ASD population [20]. Wires have been reduced or eliminated, and sensors covering several fingertips (which limited the types of activities a user could do and therefore an experimenter could study) have been rendered unnecessary with advancements in the ability to collect robust data from the wrist and transmit it wirelessly. These improvements have greatly increased our capacity for measuring on-the-go. The E4 collects signals produced by the body’s electro- dermal activity (EDA), skin temperature (SKT), and blood volume pulse (BVP) and provides a feature calculated from BVP as a continuous signal: heart rate (HR) [11]. These signals have been studied in related research [10], [20]. Based on pre- vious research [9],[12], we extract several features from these four signals, as described further in the feature extraction pro- cess section. Analyzing physiological correlates requires the use of ma- chine learning algorithms that can detect changes in these correlates with high levels of accuracy. Direct correlation anal- ysis that detects only linear relationships between signals and affect, and static rules defining when a threshold is crossed, limit the type of patterns that can define an affective state of interest [17]. However, machine learning algorithms can rec- ognize non-linear patterns and leverage high-dimensional dataspaces [9],[10],[13],[17]. An effective algorithm would: predict between affective states of interest with accuracy bet- ter than chance; have an area under the curve (AUC) as close to one as possible; and have a variance between training and testing results as close to zero as possible. The purpose of the current case study was to evaluate algorithms in the context of RAI to determine their potential efficacy in determining when students are attentive during RAI-related tasks. We addressed the following two research questions: Do physiological signals 40 IEEE Instrumentation & Measurement Magazine Authorized licensed use limited to: UNIVERSIDAD DEL ROSARIO. Downloaded on December 11,2023 at 23:28:18 UTC from IEEE Xplore. Restrictions apply. indicate different affective states of children with ASD dur- ing RAI? And How accurate are machine learning algorithms, built from physiological signals, at matching expert coders when differentiating between levels of attention by children with ASD? In this investigation, we sought to evaluate algorithms in the context of RAI to determine their potential efficacy in determining when students with ASD are attentive during RAI-related tasks. We calculated features from physiological signals collected by an E4 and compared the performance of four machine learning algorithms commonly used in affective computing. Our findings indicated the E4 data were useful for categorizing states of attentiveness in children with ASD. Method Participants and Setting We recruited participants through a university-affiliated au- tism clinic that regularly conducted social skills groups for children with ASD. We obtained parental consent and partic- ipant assent prior to the study. Two children with ASD (i.e., Cody and Max), age 11 years, participated in the investigation. Both participants were diagnosed as having ASD, using the Autism Diagnosis Observation Schedule-2, and identified to have difficulties in initiating and maintaining social interactions. Both participated in a social skills group at the autism clinic. We observed and collected data samples during 5-min ob- servations of social skills group activities that took place over several weeks. Due to the availability of only two E4s, we only included two participants in the study. Specifically, we col- lected data during a pilot evaluation of RAI and unstructured social interaction probes. During RAI, the children with ASD were seated in front of an NAO robot (Fig. 1) and directed to engage in scripted interactions. During the social interaction probes (SIP), the children with ASD were seated around a table and directed to talk with each other. Fig. 1. An NAO robot sits on a cart, ready to interact with children during the social skills intervention. May 2023 Materials and General Procedures During the investigation, the participants wore the E4 on their nondominant wrist, with the EDA electrodes on the underside of the wrist and in line between the middle and ring fingers. We used the E4 to collect data during RAI and SIP that oc- curred at the onset of each meeting of a 12-week social skills group. Upon arrival at the autism center, social skills group members, including the two participants, were brought to a multipurpose room. We placed the E4s on the participants’ wrists and similar but nonfunctional devices on the other chil- dren’s wrists. Next, we implemented RAI and SIP sessions, and then the children attended their social skills group. At the end of each group meeting, we collected E4 data during a 3-min period of quiet sitting. This daily baseline recording was gathered for use in the normalization step of processing the physiological signals. We also collected behavioral observa- tion data from video recordings of the sessions. Measurement We collected data on whether participants were attentive or inattentive during sessions using both behavioral observa- tion and physiological signals. For behavioral observations, attentiveness was defined as on-task, attending to the activ- ity, and responding to directions. Inattentive was defined as the subject being off-task, inattentive to the activity at hand, and not responding to directions. We divided observation pe- riods into 20-s intervals of our video recordings, and observers scored participants as being attentive or inattentive for each interval collected. They based their score on their observa- tion for the majority of each 20-s interval. For example, for a given 20-s video clip, if a participant exhibited a moment of be- ing on-task but was otherwise not responding to directions for the majority of the 20-s interval, then the sample was scored as inattentive. Observers were trained coders from a univer- sity research center with extensive experience in classroom observation of children with disabilities. In addition, the first author also coded a sample of 30% of the intervals to assess interrater reliability. These randomly selected intervals were selected from an equal number of samples across participants and sessions. We used Cohen’s Kappa to calculate reliability. The Cohen’s Kappa statistic is commonly used to test interra- ter reliability and is advantageous in that it accounts for chance agreement between raters [21]. We calculated interrater reli- ability to be 81.7%. Feature Extraction Process and Data Analysis Using the E4, we processed 20-s intervals to extract features that could indicate changes in affect. The features are derived from physiological signals using signal-processing tech- niques employed in previous work [12]. Data vectors were made of 12 normalized physiological features extracted from four signals. The peaks of the BVP signal were detected, and two features were calculated: mean of the peak amplitudes and maximum peak amplitude. Two features, the mean and standard deviation of HR, were calculated. For EDA, the tonic and phasic components were processed [22], and five May 2023 features extracted: mean of tonic skin activity, slope of tonic activity, peak rate of phasic activity as peaks per minute, mean of phasic peak amplitudes, and maximum peak am- plitude of phasic activity. Three features from SKT are mean, standard deviation, and slope of SKT. These features showed significant responses in previous research [9],[11],[19], and are therefore well vetted in the study of physiological signals in relation to affective computing. Moreover, previous re- search included social interaction activities for children with ASD [12], and therefore, these features are likely for consider- ation to indicate affective states during similar activities and populations. The feature extraction process provides feature vectors, each a length of 12 features. To account for day variability in physiological signals, the intervals were adjusted using the baseline from each meeting. After calculating the mean abso- lute deviation, the interval with the least variability for a given meeting day, per signal, was chosen to serve as the baseline re- cording for day variability correction of the feature vectors. The feature vectors were then min-max normalized across vec- tors, per subject and feature-wise. After accounting for day variability and normalizing the data, subject Cody had 293 fea- ture vectors, and subject Max had 250 feature vectors. These feature vectors were sent to machine learning algorithms for classification. The feature vectors for each subject were used as inputs into machine learning algorithms, which were trained to differenti- ate between the affective states of attentive and inattentive. We compared four algorithms commonly used in affective com- puting: Logistic Regression (LR), Support Vector Machines (SVM), ensemble-based Random Forest (RF), and ensemble- based Gradient Boosted Regression Trees (GBRT) algorithms. All of the machine learning models were implemented in Py- thon using scikit-learn 0.24.2. Before we built models, we split the dataset into separate training and test sets. We created sets of a typical percent- age split of 80/20 for training/test sets, after taking steps to maintain as much of the natural data as possible while also over-sampling with synthetic data to balance the classes. The test set was created by randomly shuffling and selecting 50% of the data points from the minority class, in this case the inatten- tive class. Another random shuffle selected an equal amount of data points from the majority class, in this case the atten- tive class, to create a balanced test set with natural data. Table 1 summarizes the class distributions between the original, train- ing, and test sets sent to the algorithms. The four models were fit to the training data, and we used a stratified 5-fold cross-validation scheme to prevent overfitting the model and ensuring generalization to the test data. To com- bat the class imbalance, we applied the Smote+Tomek-link (Smote+TL) over- and under-sampling technique to the train- ing split. We fit the model to the resampled training split and measured the performance using the validation split. We re- peated this process k times (i.e., 5) and averaged the results to get a performance metric. Smote+TL was implemented in Py- thon using the imbalanced-learn 0.8.0 package [23]. IEEE Instrumentation & Measurement Magazine At Authorized licensed use limited to: UNIVERSIDAD DEL ROSARIO. Downloaded on December 11,2023 at 23:28:18 UTC from IEEE Xplore. Restrictions apply. Table 1 - Class distributions in the data sets are described for each subject Data Set Original Test Training sets available Over- and Under-sampling Training sets to balance classes Attentive 222 35 187 140 (140 Natural 0 Synthetic) under-sampling Cody Class Inattentive 71 35 36 140 (36 Natural 104 Synthetic) over-sampling Max Attentive 197 26 171 104 (104 Natural 0 Synthetic) under-sampling Inattentive 53 26 27 104 (27 Natural 77 Synthetic) over-sampling sent to algorithms Several performance metrics can be used in a binary clas- sification problem, but it was important to use a metric that would capture the performance of both classes. The receiver operating characteristic (ROC) is a common method to visu- alize the performance of a binary classifier and is a reliable measure when dealing with imbalanced data. The area under the ROC curve (AUC) is a way to summarize its performance in a scalar value. AUC ranges between zero and one and indi- cates how well a classifier can separate between two classes, with closer to one indicating more accurate separation. We used AUC to evaluate the four models. Additionally, we re- ported Fl-scores for each class, along with percent variance to illustrate individual class performance as well as general performance. Results The classification performances of the four models are shown in Table 2. Each was evaluated following the criteria of choosing the model with the highest AUC and with the low- est percent variance, as well as with high Fl-scores for both classes. Based on these criteria, results from the best perform- ing models are shown in bold, namely LR for Cody and SVM for Max. Unfortunately, due to the low amount of data, it was not possible to create a model that was able to perform better than 80/20 / 140/35 140/35 104/26 104/26 Training /Test Total data sets 175 175 130 130 71.4% on at least one subject, represented by the LR model of Cody’s data. Max has two models with an AUC of 0.712, LR and SVM; however, the SVM model is chosen over the other due to the lower variance of 2.979%. In general, the highest AUC will correlate with high Fl1-scores; however, it may not be indicative of acceptable per- formance for both classes. For example, looking at the results from the RF model for Cody, the AUC is higher than that of the SVM model due to the higher F1-score of 0.74 for the at- tentive class. Conversely, the F1l-score for the inattentive class is 0.5, which is not accurate enough to be useful in practice. Model performance is specific to each subject since they were analyzed independently. Testing multiple models also proved advantageous since different models performed better for each subject. Discussion Overall, our findings indicated that E4 data are useful in cat- egorizing states of attentiveness in children with ASD and extend the available literature by evaluating the predictive effectiveness of several algorithms when applied to RAI. Fur- ther, our findings suggest that a one size fits all approach may not be effective when selecting algorithms for use with phys- iological signals. The best performing model varied across participants, highlighting the need for individualization when Table 2 —- Performance outcomes are given for four different machine learning models Cody Max Attenti Inattenti Attenti Inattenti Model ouee news AUC % Variance oe news AUC % Variance F1-score F1-score Fl1-score Fl-score LR 0.74 0.69 0.714 6.060 0.69 0.73 0.712 36.378 SVM 0.72 0.51 0.643 -5.007 0.73 0.69 0.712 2.979 RF 0.74 0.50 0.657 -3.547 0.70 0.40 0.596 -18.106 GBRT 0.71 0.43 0.614 -4.176 0.64 0.37 0.538 -24.464 42 IEEE Instrumentation & Measurement Magazine May 2023 Authorized licensed use limited to: UNIVERSIDAD DEL ROSARIO. Downloaded on December 11,2023 at 23:28:18 UTC from IEEE Xplore. Restrictions apply. processing a subject’s physiological data as well as selecting classifying algorithms. In the current investigation, we studied 12 features ex- tracted from signals collected by the E4. This approach balances between extracting over 100 features [9] and study- ing a single feature [10],[20] and is grounded in years of study supporting that these 12 are significant to affective computing research applied to children with ASD [8],[12]. Further anal- ysis, such as principal component analysis, could be used to test how much each feature impacts the classification metrics. The current offline analysis did not have to face bandwidth constraints that may arise with real-time transmission and processing of data; thus, reducing the number of signals col- lected and/or features extracted may be necessary as we move to real-time processing and closed-loop feedback during RAI. Therefore, ranking features will be part of our future work. Challenges remain for collecting enough samples, and accurate samples, for the algorithms to learn more precise thresholds between one affective state and another. Since in- dividually-designed models are often shown to outperform eroup-designed models, we would like to continue to collect data with these subjects. However, a limitation is the models trained for an individual can only assist that subject. The algorithms were able to produce better-than-chance accuracy for our affective states of interest. Furthermore, the models have F1-scores > 0.67 for both classes; therefore, these models would match a human coder on predicting attentive- ness or inattentiveness over two-thirds of the time. During future interventions, the models could be used by a robot to test anew sample of physiological data, return a prediction of affect (e.g., attentiveness or inattentiveness), and make a de- cision about actions to take during RAI. For example, if the model predicts the subject’s signals indicate attentiveness, the robot can choose to continue with practicing a social skill. However, if the model predicts inattentiveness, the robot can remind the subject about the directions and about staying on task. The current investigation lays the groundwork for future research on the use of an affect-sensitive robot to develop so- cial skills in learners with ASD. Acknowledgments The authors wish to acknowledge student researchers Janet Pulgares Soriano, Rebecca Castelly, Sabrina Daisey, Natalie Warning, Zen Kim, and Jacob Berdichevsky for their contri- butions to the research. We also want to thank the reliability coders from the College of Education and Human Develop- ment, the staff at the Norton Children’s Autism Center, the subjects, and their families. All procedures performed in stud- ies involving human participants were in accordance with the ethical standards of the institutional and/or national research committee and with the 1964 Helsinki declaration and its later amendments or comparable ethical standards. Informed con- sent was obtained from all individual participants included in the study. This research was supported by the National Science Foundation (NSF) under Smart and Connected Health (SCH) Grant #1838808 and REU Grant #1950137. May 2023 References [1] J.R. Steinbrenner et al., Evidence-Based Practices for Children, Youth, and Young Adults with Autism. The University of North Carolina at Chapel Hill: Frank Porter Graham Child Development Institute, National Clearinghouse on Autism Evidence and Practice Review Team, 2020. [2] E.E. Barton, J. E. Pustejovsky, D. M. Maggin, and B. Reichow, “Technology-aided instruction and intervention for students with ASD: a meta-analysis using novel methods of estimating effect sizes for single-case research,” Remedial and Special Ed., vol. 38, no. 6, pp. 371-386, 2017. [3] L.I. Ismail, T. Verhoeven, J. Dambre, and F. Wyffels, “Leveraging robotics research for children with autism: a review,” Int. J. Social Robotics, vol. 11, no. 3, pp. 389-410, 2019. [4] W.C. Soetal., “A robot-based play-drama intervention may improve the joint attention and functional play behaviors of Chinese-speaking preschoolers with autism spectrum disorder: a pilot study,” J. Autism and Developmental Disorders, vol. 50, no. 2, pp. 467-481, 2020. [5] R. Pennington, M. N. Saadatzi, K. C. Welch, and R. Scott, “Using robot-assisted instruction to teach students with intellectual disabilities to use personal narrative in text messages,” J. Special Fd. Technol., vol. 29, no. 4, pp. 49-58, 2014. [6] [6] FR Marino et al., “Outcomes of a robot-assisted social-emotional understanding intervention for young children with autism spectrum disorders,” J. Autism and Developmental Disorders, vol. 50, no. 6, pp. 1973-1987, 2020. [7] M. W. De Korte et al., “Self-initiations in young children with autism during pivotal response treatment with and without robot assistance,” Autism, vol. 24, no. 8, pp. 2117-2128, 2020. [8] K.C. Welch, “Physiological signals of autistic children can be useful,” IEEE Instrum. Meas. Mag., vol. 15, no. 1, pp. 28-32, Feb. 2012. [9] O. AlZoubi, S. D’Mello, and R. A. Calvo, “Detecting naturalistic expressions of nonbasic affect using physiological signals,” IEEE Trans. Affect. Comput., vol. 3, no. 3, Jul.-Sep. 2012. [10] R. Assabumrunegrat et al., “Ubiquitous affective computing: a review,” IEEE Sens. J., vol. 22, no. 3, pp. 1867-1881, Feb. 2022. [11] C. Saitis and K. Kalimeri, “Multimodal classification of stressful environments in visually impaired mobility using EEG and peripheral biosignals,” IEEE Trans. Affect. Comput., vol. 12, no. 1, pp. 203-214, Jan.-Mar. 2021. [12] K. C. Welch, U. Lahiri, Z. W. Warren, and N. Sarkar, “A system to measure physiological response during social interaction in VR for children with ASD,” in Computational Models for Biomedical Reasoning and Problem Solving, C. Chen & S. Cheung, Eds. Hershey, Pennsylvania, USA: IGI Global, pp. 1-33, 2019. [13] S. D’Mello, A. Kappas, and J. Gratch, “The affective computing approach to affect measurement,” Emotion Rev., vol. 10, no. 2, pp. 174-183, Apr. 2018. [14] K. C. Welch, C. Harnett, and Y-C. Lee, “A review on measuring affect with practical sensors to monitor driver behavior,” Safety, vol. 5, no. 4, pp. 72-89, 2019. [15] H. Lee, S. Kang, and U. Lee, “Understanding privacy risks and perceived benefits in open dataset collection for mobile affective computing,” in Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., vol. 6, no. 2, Jun. 2022. IEEE Instrumentation & Measurement Magazine 43 Authorized licensed use limited to: UNIVERSIDAD DEL ROSARIO. Downloaded on December 11,2023 at 23:28:18 UTC from IEEE Xplore. Restrictions apply. [16] G. Cosoli, S. Spinsante, and L. Scalise, “Wearable devices and diagnostic apps: beyond the borders of traditional medicine, but what about their accuracy and reliability?” IEEE Instrum. Meas. Mag., vol. 24, no. 6, pp. 89-94, Sep. 2021. [17] B. Rush, L. A. Celi, and D. J. Stone, “Applying machine learning to continuously monitored physiological data,” J. Clin. Monit. Comput., vol. 33, no. 5, pp. 887-893, Oct. 2019. [18] M. Garbarino, M. Lai, D. Bender, R. W. Picard, and S. Tognetti, “Empatica E3 - A wearable wireless multi-sensor device for real- time computerized biofeedback and data acquisition,” in Proc. EAI Mobihealth, pp. 39-42, 2014. [19] M. Sevil, M. Rashid, M. R. Askari, Z. Maloney, I. Hajizadeh, and A. Cinar, “Detection and characterization of physical activity and psychological stress from wristband data,” Signals, vol. 1, no. 2, pp. 188-208, 2020. [20] D. Ahuja, A. Sarkar, S. Chandra, and P. Kumar, “Wearable technology for monitoring behavioral and physiological responses in children with autism spectrum disorder: A literature review,” Tech. Dis., vol. 34, no. 2, pp. 69-84, 2022. [21] M. L. McHugh, “Interrater reliability: the kappa statistic,” Biochem. Med., vol. 22, no. 3, pp. 276-282, 2012. [22] A. Bizzego, A. Battisti, G. Gabrieli, G. Esposito, and C. Furlanello, “pyphysio: A physiological signal processing library for data science approaches in physiology,” SoftwarexX, vol. 10, 2019. [23] G. Lemaitre, F. Nogueira, and C. K. Aridas, “Imbalanced-learn: a python toolbox to tackle the curse of imbalanced datasets in machine learning,” J. Machine Learning Research, vol. 18, no. 17, pp. 1-5, 2017. Karla Conn Welch (karla.welch@louisville.edu) is an Asso- ciate Professor in the Electrical and Computer Engineering Department at the University of Louisville in Louisville, Ken- tucky. She received the M.S. degree and the Ph.D. degree in electrical engineering and computer science from Vanderbilt University in Nashville, Tennessee in 2005 and 2009, respec- tively. Her research focus areas include physiological signal processing, machine learning, affective computing, and hu- man-robot interaction. Robert Pennington, PhD BCBA-D, is the Lake and Edward J Snyder, Jr. Distinguished Professor in Special Education at the University of North Carolina at Charlotte. His current research interests involve behavior analytic communication instruc- tion, expanding students’ repertoires in written expression, 44 IEEE Instrumentation & Measurement Magazine Authorized licensed use limited to: UNIVERSIDAD DEL ROSARIO. Downloaded on December 11,2023 at 23:28:18 UTC from IEEE Xplore. Restrictions apply. technology-aided intervention, and improving educational programming for students with severe disabilities. Saipruthvi Vanaparthy is an Electronics Design Engineer, de- veloping high-speed controls systems for photolithography machines at ASML in San Diego, California. He received his M.S. degree in electrical engineering at the University of Lou- isville in 2021. At the time of this study, he was a Graduate Research Assistant in the Electrical and Computer Engineer- ing Department at the University of Louisville in Louisville, Kentucky. Ha Manh Do is a Senior Machine Learning Engineer with Plume Design in Palo Alto, California. He received his Ph.D. degree in electrical engineering at Oklahoma State Univer- sity in Stillwater, Oklahoma in 2018. At the time of this study, he was a Postdoctoral Researcher in the Electrical and Com- puter Engineering Department at the University of Louisville in Louisville, Kentucky. His research interests include human- robot interaction, applied artificial intelligence, and machine learning. Rohit Narayanan is an Undergraduate Student in the Department of Electrical and Computer Engineering at Princeton University in Princeton, New Jersey. At the time of this study, he was a visiting Undergraduate Research As- sistant in the Louisville Automation and Robotics Research Institute at the University of Louisville in Louisville, Ken- tucky. His research interests are soft robotics and embedded systems. Dan Popa is Director of the Louisville Automation and Ro- botics Research Institute and a Professor in the Electrical and Computer Engineering Department at the University of Lou- isville in Louisville, Kentucky. Gregory Barnes is Director of the Norton Children’s Autism Center and a Professor in the Department of Neurology at the University of Louisville in Louisville, Kentucky. Grace Kuravackel is a Clinical Psychologist in the Norton Children’s Autism Center and an Associate Professor in the Department of Pediatrics at the University of Louisville in Louisville, Kentucky. May 2023 For Further Reading R. Saravanakumar et al., “IoB: sensors for wearable monitoring and enhancing health care systems,” IEEE Instrum. Meas. Mag., vol. 25, no. 3, pp. 63-70, May 2022. V. Dinut, “Biofeedback in psychotherapy [Trends in Future I&M],” IEEE Instrum. Meas. Mag., vol. 20, no. 2, pp. 31-32, Apr. 2017. L. Florea, “Future trends in early diagnosis for cognition impairments in children based on eye measurements [Trends in Future I&M],” IEEE Trans. Instrum. Meas., vol. 21, no. 3, pp. 41-42, Jun. 2018. S. Singh, M. Koztowski, I. Garcia-Lopez, Z. Jiang and E. Rodriguez-Villegas, “Proof of concept of a novel neck-situ- ated wearable PPG system for continuous physiological monitoring,” IEEE Trans. Instrum. Meas., vol. 70, pp. 1-9, 2021. W. L. Woo, “Human-machine co-creation in the rise of AI [Trends in Future I&M],” IEEE Instrum. Meas. Mag., vol. 23, no. 2, pp. 71-73, Apr. 2020. M. Choi, G. Koo, M. Seo and S. W. Kim, “Wearable device-based system to monitor a driver’s stress, fatigue, and drowsiness,” [EEE Trans. Instrum. Meas., vol. 67, no. 3, pp. 634-645, Mar. 2018. May 2023 IEEE Instrumentation & Measurement Magazine 45 Authorized licensed use limited to: UNIVERSIDAD DEL ROSARIO. Downloaded on December 11,2023 at 23:28:18 UTC from IEEE Xplore. Restrictions apply. 